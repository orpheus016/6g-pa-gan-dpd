{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "139488e6",
   "metadata": {},
   "source": [
    "# üéØ 6G PA DPD Training Notebook\n",
    "## CWGAN-GP + Spectral Loss + QAT for 29th LSI Design Contest\n",
    "\n",
    "This notebook trains a **Time-Delay Neural Network (TDNN)** for Digital Predistortion using:\n",
    "- **OpenDPD APA_200MHz**: Real measured 200MHz PA data from GitHub\n",
    "- **CWGAN-GP**: Conditional Wasserstein GAN with Gradient Penalty\n",
    "- **Spectral Loss**: EVM + ACPR optimization (mimics spectrum analyzer)\n",
    "- **QAT**: Quantization-Aware Training for FPGA deployment (Q1.15 weights, Q8.8 activations)\n",
    "- **Thermal Robustness**: Cold/Normal/Hot weight variants\n",
    "\n",
    "### Architecture\n",
    "```\n",
    "Input (18) ‚Üí FC(32) ‚Üí LeakyReLU ‚Üí FC(16) ‚Üí LeakyReLU ‚Üí FC(2) ‚Üí Tanh ‚Üí Output\n",
    "Parameters: 1,170 (vs 10,000+ for memory polynomial at 6G BW)\n",
    "```\n",
    "\n",
    "**Target**: PYNQ-Z1 / ZCU104 FPGA with HDMI loopback demo\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Expected Results\n",
    "- **No DPD**: ACPR ~ -35 dBc, EVM ~ 8%\n",
    "- **Supervised (MSE only)**: ACPR ~ -45 dBc, EVM ~ 4%\n",
    "- **GAN + Spectral Loss**: ACPR ~ -48 dBc, EVM ~ 3.5% ‚úÖ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e95ba60",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72607f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository (run once)\n",
    "!git clone https://github.com/orpheus016/6g-pa-gan-dpd.git 2>/dev/null || echo \"Already cloned\"\n",
    "%cd 6g-pa-gan-dpd\n",
    "\n",
    "# Install minimal dependencies (most are pre-installed on Colab)\n",
    "!pip install -q h5py pyyaml tqdm scipy\n",
    "\n",
    "print(\"‚úÖ Environment ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab2fe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import yaml\n",
    "import scipy.io\n",
    "import scipy.signal\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "print(\"‚úÖ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e2c73e",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c37341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "with open('config/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Display key parameters\n",
    "print(\"=\" * 50)\n",
    "print(\"Configuration Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Sample Rate: {config['system']['sample_rate']/1e6:.0f} MHz\")\n",
    "print(f\"TDNN Architecture: {config['model']['generator']['input_dim']} ‚Üí \"\n",
    "      f\"{' ‚Üí '.join(map(str, config['model']['generator']['hidden_dims']))} ‚Üí \"\n",
    "      f\"{config['model']['generator']['output_dim']}\")\n",
    "print(f\"Quantization: {config['quantization']['weight_bits']}-bit weights\")\n",
    "print(f\"Batch Size: {config['training']['batch_size']}\")\n",
    "print(f\"Learning Rate: {config['training']['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70add694",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Download OpenDPD Dataset from GitHub\n",
    "\n",
    "We'll download the **APA_200MHz.mat** dataset directly from the OpenDPD repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e935ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory\n",
    "!mkdir -p data\n",
    "\n",
    "# Download OpenDPD APA_200MHz dataset (290KB - very small!)\n",
    "print(\"üì• Downloading OpenDPD APA_200MHz.mat dataset...\")\n",
    "!wget -q -O data/APA_200MHz.mat https://github.com/lab-emi/OpenDPD/raw/main/datasets/APA_200MHz/APA_200MHz.mat\n",
    "\n",
    "# Verify download\n",
    "import os\n",
    "file_size = os.path.getsize('data/APA_200MHz.mat') / 1024\n",
    "print(f\"‚úÖ Downloaded: {file_size:.1f} KB\")\n",
    "\n",
    "if file_size < 100:\n",
    "    print(\"‚ö†Ô∏è File seems too small, download may have failed\")\n",
    "else:\n",
    "    print(\"‚úÖ Dataset ready for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a40dfae",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Inspect OpenDPD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5222e762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect OpenDPD data\n",
    "mat_data = scipy.io.loadmat('data/APA_200MHz.mat')\n",
    "\n",
    "print(\"üì¶ Dataset contents:\")\n",
    "for key in mat_data.keys():\n",
    "    if not key.startswith('__'):\n",
    "        data = mat_data[key]\n",
    "        if hasattr(data, 'shape'):\n",
    "            print(f\"  {key}: shape={data.shape}, dtype={data.dtype}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {type(data)}\")\n",
    "\n",
    "# Extract input/output signals\n",
    "if 'u' in mat_data and 'y' in mat_data:\n",
    "    u_pa = mat_data['u'].flatten()  # PA input (clean signal)\n",
    "    y_pa = mat_data['y'].flatten()  # PA output (distorted)\n",
    "    print(f\"\\n‚úÖ Found PA input/output signals\")\n",
    "    print(f\"   Samples: {len(u_pa):,}\")\n",
    "    print(f\"   PA input power: {10*np.log10(np.mean(np.abs(u_pa)**2)):.2f} dBm (normalized)\")\n",
    "    print(f\"   PA output power: {10*np.log10(np.mean(np.abs(y_pa)**2)):.2f} dBm (normalized)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Expected 'u' and 'y' fields not found. Available keys:\", [k for k in mat_data.keys() if not k.startswith('__')])\n",
    "    # Try alternative field names\n",
    "    possible_inputs = [k for k in mat_data.keys() if not k.startswith('__')]\n",
    "    if len(possible_inputs) >= 2:\n",
    "        u_pa = mat_data[possible_inputs[0]].flatten()\n",
    "        y_pa = mat_data[possible_inputs[1]].flatten()\n",
    "        print(f\"   Using {possible_inputs[0]} as input, {possible_inputs[1]} as output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aff15f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PA characteristics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# AM/AM curve (gain compression)\n",
    "ax = axes[0, 0]\n",
    "input_mag = np.abs(u_pa)\n",
    "output_mag = np.abs(y_pa)\n",
    "ax.scatter(input_mag[:5000], output_mag[:5000], alpha=0.3, s=1)\n",
    "ax.set_xlabel('Input Magnitude')\n",
    "ax.set_ylabel('Output Magnitude')\n",
    "ax.set_title('AM/AM Curve (Gain Compression)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# AM/PM curve (phase distortion)\n",
    "ax = axes[0, 1]\n",
    "phase_shift = np.angle(y_pa) - np.angle(u_pa)\n",
    "ax.scatter(input_mag[:5000], np.rad2deg(phase_shift[:5000]), alpha=0.3, s=1, c='orange')\n",
    "ax.set_xlabel('Input Magnitude')\n",
    "ax.set_ylabel('Phase Shift (degrees)')\n",
    "ax.set_title('AM/PM Curve (Phase Distortion)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Input spectrum\n",
    "ax = axes[1, 0]\n",
    "f_in, psd_in = scipy.signal.welch(u_pa, fs=200e6, nperseg=1024, return_onesided=False)\n",
    "f_in = np.fft.fftshift(f_in)\n",
    "psd_in = np.fft.fftshift(psd_in)\n",
    "ax.plot(f_in / 1e6, 10*np.log10(psd_in + 1e-12), linewidth=1)\n",
    "ax.set_xlabel('Frequency (MHz)')\n",
    "ax.set_ylabel('PSD (dB)')\n",
    "ax.set_title('Input Spectrum (Clean)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim([-80, 0])\n",
    "\n",
    "# Output spectrum (with distortion)\n",
    "ax = axes[1, 1]\n",
    "f_out, psd_out = scipy.signal.welch(y_pa, fs=200e6, nperseg=1024, return_onesided=False)\n",
    "f_out = np.fft.fftshift(f_out)\n",
    "psd_out = np.fft.fftshift(psd_out)\n",
    "ax.plot(f_out / 1e6, 10*np.log10(psd_out + 1e-12), linewidth=1, color='red')\n",
    "ax.set_xlabel('Frequency (MHz)')\n",
    "ax.set_ylabel('PSD (dB)')\n",
    "ax.set_title('Output Spectrum (Distorted - shows spectral regrowth)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim([-80, 0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('pa_characteristics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ PA characteristics visualized\")\n",
    "print(\"   Notice spectral regrowth in output ‚Üí DPD must fix this!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49c0460",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Define Models with QAT Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f86a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDNNGenerator(nn.Module):\n",
    "    \"\"\"Time-Delay Neural Network for DPD with Quantization-Aware Training\"\"\"\n",
    "    def __init__(self, memory_depth=5, hidden1=32, hidden2=16, quantize=False):\n",
    "        super().__init__()\n",
    "        self.memory_depth = memory_depth\n",
    "        self.input_size = 2 + 2 * memory_depth * 2  # I, Q + delayed I, Q, |x|^2*I, |x|^2*Q\n",
    "        self.quantize = quantize\n",
    "        \n",
    "        # Network layers\n",
    "        self.fc1 = nn.Linear(self.input_size, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, 2)\n",
    "        self.lrelu = nn.LeakyReLU(0.2)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        # QAT: Fake quantization for weights (Q1.15) and activations (Q8.8)\n",
    "        if quantize:\n",
    "            self.weight_quant = lambda x: torch.clamp(torch.round(x * 32768) / 32768, -1, 1-1/32768)\n",
    "            self.act_quant = lambda x: torch.clamp(torch.round(x * 256) / 256, -128, 128-1/256)\n",
    "        \n",
    "    def create_input_features(self, x):\n",
    "        \"\"\"Create TDNN input: [I, Q, I_{t-1}, Q_{t-1}, ..., |x|^2*I, |x|^2*Q, ...]\"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        features = [x]  # Current sample\n",
    "        \n",
    "        # Add delayed samples\n",
    "        for d in range(1, self.memory_depth + 1):\n",
    "            if d < x.shape[0]:\n",
    "                delayed = torch.cat([torch.zeros(d, 2, device=x.device), x[:-d]], dim=0)\n",
    "            else:\n",
    "                delayed = torch.zeros_like(x)\n",
    "            features.append(delayed)\n",
    "            \n",
    "            # Add nonlinear terms: |x|^2 * x\n",
    "            mag_sq = (delayed[:, 0]**2 + delayed[:, 1]**2).unsqueeze(1)\n",
    "            features.append(mag_sq * delayed)\n",
    "        \n",
    "        return torch.cat(features, dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Create input features\n",
    "        x = self.create_input_features(x)\n",
    "        \n",
    "        # Layer 1\n",
    "        if self.quantize:\n",
    "            w1 = self.weight_quant(self.fc1.weight)\n",
    "            x = nn.functional.linear(x, w1, self.fc1.bias)\n",
    "            x = self.act_quant(self.lrelu(x))\n",
    "        else:\n",
    "            x = self.lrelu(self.fc1(x))\n",
    "        \n",
    "        # Layer 2\n",
    "        if self.quantize:\n",
    "            w2 = self.weight_quant(self.fc2.weight)\n",
    "            x = nn.functional.linear(x, w2, self.fc2.bias)\n",
    "            x = self.act_quant(self.lrelu(x))\n",
    "        else:\n",
    "            x = self.lrelu(self.fc2(x))\n",
    "        \n",
    "        # Layer 3\n",
    "        if self.quantize:\n",
    "            w3 = self.weight_quant(self.fc3.weight)\n",
    "            x = nn.functional.linear(x, w3, self.fc3.bias)\n",
    "        else:\n",
    "            x = self.fc3(x)\n",
    "        \n",
    "        return self.tanh(x)\n",
    "\n",
    "print(\"‚úÖ TDNN Generator defined with QAT support\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c035978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Critic for CWGAN-GP (distinguishes real vs fake PA outputs)\"\"\"\n",
    "    def __init__(self, input_size=2, hidden_sizes=[64, 32]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_size = input_size\n",
    "        \n",
    "        for h in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(in_size, h),\n",
    "                nn.LayerNorm(h),\n",
    "                nn.LeakyReLU(0.2)\n",
    "            ])\n",
    "            in_size = h\n",
    "        \n",
    "        layers.append(nn.Linear(in_size, 1))  # No sigmoid (Wasserstein)\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "print(\"‚úÖ Discriminator (Critic) defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9863059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PADigitalTwin(nn.Module):\n",
    "    \"\"\"Digital twin of PA for ILA (Indirect Learning Architecture)\"\"\"\n",
    "    def __init__(self, memory_depth=5):\n",
    "        super().__init__()\n",
    "        self.memory_depth = memory_depth\n",
    "        input_size = 2 + 2 * memory_depth * 2\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(32, 2),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def create_input_features(self, x):\n",
    "        \"\"\"Same feature extraction as TDNN\"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        features = [x]\n",
    "        \n",
    "        for d in range(1, self.memory_depth + 1):\n",
    "            if d < x.shape[0]:\n",
    "                delayed = torch.cat([torch.zeros(d, 2, device=x.device), x[:-d]], dim=0)\n",
    "            else:\n",
    "                delayed = torch.zeros_like(x)\n",
    "            features.append(delayed)\n",
    "            mag_sq = (delayed[:, 0]**2 + delayed[:, 1]**2).unsqueeze(1)\n",
    "            features.append(mag_sq * delayed)\n",
    "        \n",
    "        return torch.cat(features, dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.create_input_features(x)\n",
    "        return self.model(x)\n",
    "\n",
    "print(\"‚úÖ PA Digital Twin defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40539d8e",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Define Spectral Loss Functions\n",
    "\n",
    "These functions compute **EVM** and **ACPR** - the same metrics a spectrum analyzer would show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a747cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_evm(signal, reference, db=True):\n",
    "    \"\"\"Compute Error Vector Magnitude\"\"\"\n",
    "    error = signal - reference\n",
    "    evm_rms = torch.sqrt(torch.mean(torch.abs(error)**2)) / torch.sqrt(torch.mean(torch.abs(reference)**2))\n",
    "    if db:\n",
    "        return 20 * torch.log10(evm_rms + 1e-12)\n",
    "    return evm_rms * 100  # percentage\n",
    "\n",
    "def compute_acpr(signal, fs=200e6, bw_main=50e6, bw_adj=50e6, offset=75e6):\n",
    "    \"\"\"Compute Adjacent Channel Power Ratio\"\"\"\n",
    "    # FFT\n",
    "    N = signal.shape[0]\n",
    "    spectrum = torch.fft.fft(signal.view(-1))\n",
    "    psd = torch.abs(spectrum)**2\n",
    "    freqs = torch.fft.fftfreq(N, 1/fs)\n",
    "    \n",
    "    # Main channel power\n",
    "    main_mask = torch.abs(freqs) < bw_main / 2\n",
    "    p_main = torch.sum(psd[main_mask])\n",
    "    \n",
    "    # Adjacent channel power (upper + lower)\n",
    "    adj_upper_mask = (freqs > offset - bw_adj/2) & (freqs < offset + bw_adj/2)\n",
    "    adj_lower_mask = (freqs > -offset - bw_adj/2) & (freqs < -offset + bw_adj/2)\n",
    "    p_adj = torch.sum(psd[adj_upper_mask]) + torch.sum(psd[adj_lower_mask])\n",
    "    \n",
    "    # ACPR in dB\n",
    "    acpr = 10 * torch.log10((p_adj / 2) / (p_main + 1e-12) + 1e-12)\n",
    "    return acpr\n",
    "\n",
    "def spectral_loss(output, target, evm_weight=1.0, acpr_weight=0.1, fs=200e6):\n",
    "    \"\"\"Combined EVM + ACPR loss\"\"\"\n",
    "    evm = compute_evm(output, target, db=False)  # Use percentage for gradient stability\n",
    "    \n",
    "    # ACPR is expensive, compute on subset\n",
    "    if output.shape[0] > 1024:\n",
    "        acpr = compute_acpr(output[:1024], fs=fs)\n",
    "    else:\n",
    "        acpr = compute_acpr(output, fs=fs)\n",
    "    \n",
    "    # Lower ACPR is better (more negative), so we minimize -ACPR\n",
    "    loss = evm_weight * evm - acpr_weight * acpr\n",
    "    return loss, evm, acpr\n",
    "\n",
    "print(\"‚úÖ Spectral loss functions defined (EVM + ACPR)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f723dca4",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Prepare Training Data with Thermal Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f02a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "u_tensor = torch.from_numpy(np.stack([u_pa.real, u_pa.imag], axis=1).astype(np.float32)).to(device)\n",
    "y_tensor = torch.from_numpy(np.stack([y_pa.real, y_pa.imag], axis=1).astype(np.float32)).to(device)\n",
    "\n",
    "print(f\"üìä Training data prepared:\")\n",
    "print(f\"   Input shape: {u_tensor.shape}\")\n",
    "print(f\"   Output shape: {y_tensor.shape}\")\n",
    "print(f\"   Device: {device}\")\n",
    "\n",
    "# Split into train/val\n",
    "train_size = int(0.8 * len(u_tensor))\n",
    "u_train, u_val = u_tensor[:train_size], u_tensor[train_size:]\n",
    "y_train, y_val = y_tensor[:train_size], y_tensor[train_size:]\n",
    "\n",
    "print(f\"\\n   Train samples: {len(u_train):,}\")\n",
    "print(f\"   Val samples: {len(u_val):,}\")\n",
    "\n",
    "# Thermal augmentation function\n",
    "def add_thermal_drift(signal, temp_variant='normal'):\n",
    "    \"\"\"Simulate PA thermal drift for robust DPD\"\"\"\n",
    "    if temp_variant == 'cold':  # -10¬∞C\n",
    "        gain_shift = 1.05\n",
    "        phase_shift = -0.02  # radians\n",
    "    elif temp_variant == 'hot':  # +60¬∞C\n",
    "        gain_shift = 0.95\n",
    "        phase_shift = 0.03\n",
    "    else:  # normal (25¬∞C)\n",
    "        return signal\n",
    "    \n",
    "    # Apply gain and phase shift\n",
    "    magnitude = torch.sqrt(signal[:, 0]**2 + signal[:, 1]**2)\n",
    "    phase = torch.atan2(signal[:, 1], signal[:, 0]) + phase_shift\n",
    "    \n",
    "    augmented = torch.stack([\n",
    "        gain_shift * magnitude * torch.cos(phase),\n",
    "        gain_shift * magnitude * torch.sin(phase)\n",
    "    ], dim=1)\n",
    "    \n",
    "    return augmented\n",
    "\n",
    "print(\"\\n‚úÖ Thermal augmentation ready (will generate Cold/Normal/Hot variants)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0556c3",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Train CWGAN-GP with Spectral Loss\n",
    "\n",
    "**Architecture:**\n",
    "- **Generator (TDNN)**: Learns DPD pre-distortion\n",
    "- **Discriminator**: Distinguishes real PA output from DPD+PA output\n",
    "- **Spectral Loss**: Directly optimizes EVM and ACPR\n",
    "- **ILA**: Indirect Learning Architecture (train on PA output, not input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd80911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "BATCH_SIZE = 256\n",
    "LR_G = 1e-4\n",
    "LR_D = 1e-4\n",
    "N_EPOCHS = 50\n",
    "N_CRITIC = 5  # Train discriminator 5x per generator update\n",
    "LAMBDA_GP = 10  # Gradient penalty weight\n",
    "LAMBDA_SPECTRAL = 0.5  # Spectral loss weight\n",
    "ENABLE_QAT_EPOCH = 30  # Start quantization-aware training at epoch 30\n",
    "\n",
    "print(f\"üéØ Training configuration:\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Learning rate: G={LR_G}, D={LR_D}\")\n",
    "print(f\"   Epochs: {N_EPOCHS}\")\n",
    "print(f\"   QAT starts at epoch: {ENABLE_QAT_EPOCH}\")\n",
    "print(f\"   Spectral loss weight: {LAMBDA_SPECTRAL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f7f77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "generator = TDNNGenerator(memory_depth=5, hidden1=32, hidden2=16, quantize=False).to(device)\n",
    "discriminator = Discriminator(input_size=2, hidden_sizes=[64, 32]).to(device)\n",
    "pa_twin = PADigitalTwin(memory_depth=5).to(device)\n",
    "\n",
    "# Optimizers\n",
    "opt_g = torch.optim.Adam(generator.parameters(), lr=LR_G, betas=(0.5, 0.999))\n",
    "opt_d = torch.optim.Adam(discriminator.parameters(), lr=LR_D, betas=(0.5, 0.999))\n",
    "opt_pa = torch.optim.Adam(pa_twin.parameters(), lr=1e-3)\n",
    "\n",
    "print(f\"\\nüìê Model parameters:\")\n",
    "print(f\"   Generator: {sum(p.numel() for p in generator.parameters()):,}\")\n",
    "print(f\"   Discriminator: {sum(p.numel() for p in discriminator.parameters()):,}\")\n",
    "print(f\"   PA Twin: {sum(p.numel() for p in pa_twin.parameters()):,}\")\n",
    "\n",
    "print(\"\\n‚úÖ Models initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff63b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-train PA digital twin to match real PA\n",
    "print(\"\\nüîß Pre-training PA digital twin...\")\n",
    "pa_twin.train()\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for i in range(0, len(u_train), BATCH_SIZE):\n",
    "        batch_u = u_train[i:i+BATCH_SIZE]\n",
    "        batch_y = y_train[i:i+BATCH_SIZE]\n",
    "        \n",
    "        # Train PA twin to match real PA\n",
    "        pa_pred = pa_twin(batch_u)\n",
    "        loss = nn.MSELoss()(pa_pred, batch_y)\n",
    "        \n",
    "        opt_pa.zero_grad()\n",
    "        loss.backward()\n",
    "        opt_pa.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / (len(u_train) // BATCH_SIZE)\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        print(f\"   Epoch {epoch+1}/10: Loss={avg_loss:.6f}\")\n",
    "\n",
    "print(\"‚úÖ PA twin pre-trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d89d780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "history = {\n",
    "    'g_loss': [], 'd_loss': [], 'evm': [], 'acpr': [],\n",
    "    'val_evm': [], 'val_acpr': []\n",
    "}\n",
    "\n",
    "print(\"\\nüöÄ Starting CWGAN-GP training with Spectral Loss...\\n\")\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    # Enable QAT after specified epoch\n",
    "    if epoch == ENABLE_QAT_EPOCH:\n",
    "        print(f\"\\nüî¢ Enabling Quantization-Aware Training at epoch {epoch+1}\")\n",
    "        generator.quantize = True\n",
    "    \n",
    "    epoch_g_loss = 0\n",
    "    epoch_d_loss = 0\n",
    "    epoch_evm = 0\n",
    "    epoch_acpr = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    pbar = tqdm(range(0, len(u_train) - BATCH_SIZE, BATCH_SIZE), desc=f\"Epoch {epoch+1}/{N_EPOCHS}\")\n",
    "    \n",
    "    for i in pbar:\n",
    "        batch_u = u_train[i:i+BATCH_SIZE]\n",
    "        batch_y = y_train[i:i+BATCH_SIZE]\n",
    "        batch_size = batch_u.shape[0]\n",
    "        \n",
    "        # ==================== Train Discriminator ====================\n",
    "        for _ in range(N_CRITIC):\n",
    "            # Generate pre-distorted signal\n",
    "            dpd_out = generator(batch_u)\n",
    "            \n",
    "            # Pass through PA twin (ILA architecture)\n",
    "            fake_pa_out = pa_twin(dpd_out)\n",
    "            \n",
    "            # Discriminator predictions\n",
    "            real_pred = discriminator(batch_y)\n",
    "            fake_pred = discriminator(fake_pa_out.detach())\n",
    "            \n",
    "            # Wasserstein loss\n",
    "            d_loss = -(torch.mean(real_pred) - torch.mean(fake_pred))\n",
    "            \n",
    "            # Gradient penalty\n",
    "            alpha = torch.rand(batch_size, 1, device=device)\n",
    "            interpolates = (alpha * batch_y + (1 - alpha) * fake_pa_out.detach()).requires_grad_(True)\n",
    "            d_interpolates = discriminator(interpolates)\n",
    "            \n",
    "            gradients = torch.autograd.grad(\n",
    "                outputs=d_interpolates,\n",
    "                inputs=interpolates,\n",
    "                grad_outputs=torch.ones_like(d_interpolates),\n",
    "                create_graph=True,\n",
    "                retain_graph=True\n",
    "            )[0]\n",
    "            \n",
    "            gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "            d_loss_total = d_loss + LAMBDA_GP * gradient_penalty\n",
    "            \n",
    "            opt_d.zero_grad()\n",
    "            d_loss_total.backward()\n",
    "            opt_d.step()\n",
    "        \n",
    "        # ==================== Train Generator ====================\n",
    "        dpd_out = generator(batch_u)\n",
    "        fake_pa_out = pa_twin(dpd_out)\n",
    "        \n",
    "        # Adversarial loss\n",
    "        g_adv_loss = -torch.mean(discriminator(fake_pa_out))\n",
    "        \n",
    "        # Spectral loss (EVM + ACPR)\n",
    "        spec_loss, evm, acpr = spectral_loss(fake_pa_out, batch_u, evm_weight=1.0, acpr_weight=0.1)\n",
    "        \n",
    "        # Total generator loss\n",
    "        g_loss = g_adv_loss + LAMBDA_SPECTRAL * spec_loss\n",
    "        \n",
    "        opt_g.zero_grad()\n",
    "        g_loss.backward()\n",
    "        opt_g.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        epoch_g_loss += g_loss.item()\n",
    "        epoch_d_loss += d_loss.item()\n",
    "        epoch_evm += evm.item()\n",
    "        epoch_acpr += acpr.item()\n",
    "        n_batches += 1\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'G': f'{g_loss.item():.3f}',\n",
    "            'D': f'{d_loss.item():.3f}',\n",
    "            'EVM': f'{evm.item():.2f}%',\n",
    "            'ACPR': f'{acpr.item():.1f}dB'\n",
    "        })\n",
    "    \n",
    "    # Epoch summary\n",
    "    avg_g_loss = epoch_g_loss / n_batches\n",
    "    avg_d_loss = epoch_d_loss / n_batches\n",
    "    avg_evm = epoch_evm / n_batches\n",
    "    avg_acpr = epoch_acpr / n_batches\n",
    "    \n",
    "    history['g_loss'].append(avg_g_loss)\n",
    "    history['d_loss'].append(avg_d_loss)\n",
    "    history['evm'].append(avg_evm)\n",
    "    history['acpr'].append(avg_acpr)\n",
    "    \n",
    "    # Validation\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        val_dpd = generator(u_val)\n",
    "        val_pa_out = pa_twin(val_dpd)\n",
    "        val_spec_loss, val_evm, val_acpr = spectral_loss(val_pa_out, u_val)\n",
    "    \n",
    "    history['val_evm'].append(val_evm.item())\n",
    "    history['val_acpr'].append(val_acpr.item())\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{N_EPOCHS} Summary:\")\n",
    "    print(f\"  Train - EVM: {avg_evm:.2f}%, ACPR: {avg_acpr:.1f} dBc\")\n",
    "    print(f\"  Val   - EVM: {val_evm.item():.2f}%, ACPR: {val_acpr.item():.1f} dBc\")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c615a07",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41f6022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Loss curves\n",
    "ax = axes[0, 0]\n",
    "ax.plot(history['g_loss'], label='Generator', linewidth=2)\n",
    "ax.plot(history['d_loss'], label='Discriminator', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('GAN Training Loss')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# EVM\n",
    "ax = axes[0, 1]\n",
    "ax.plot(history['evm'], label='Train EVM', linewidth=2)\n",
    "ax.plot(history['val_evm'], label='Val EVM', linewidth=2, linestyle='--')\n",
    "ax.axhline(y=3.5, color='g', linestyle=':', label='Target 3.5%')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('EVM (%)')\n",
    "ax.set_title('Error Vector Magnitude')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# ACPR\n",
    "ax = axes[1, 0]\n",
    "ax.plot(history['acpr'], label='Train ACPR', linewidth=2)\n",
    "ax.plot(history['val_acpr'], label='Val ACPR', linewidth=2, linestyle='--')\n",
    "ax.axhline(y=-48, color='g', linestyle=':', label='Target -48 dBc')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('ACPR (dBc)')\n",
    "ax.set_title('Adjacent Channel Power Ratio')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# QAT marker\n",
    "ax = axes[1, 1]\n",
    "ax.plot(history['val_evm'], label='Val EVM', linewidth=2)\n",
    "ax.axvline(x=ENABLE_QAT_EPOCH, color='r', linestyle='--', label=f'QAT Start (Epoch {ENABLE_QAT_EPOCH})')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Validation EVM (%)')\n",
    "ax.set_title('Impact of Quantization-Aware Training')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Final Results:\")\n",
    "print(f\"   Best Val EVM: {min(history['val_evm']):.2f}%\")\n",
    "print(f\"   Best Val ACPR: {min(history['val_acpr']):.1f} dBc\")\n",
    "print(f\"\\n‚úÖ Training visualized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1126559b",
   "metadata": {},
   "source": [
    "## üîü Generate Thermal Variants (Cold/Normal/Hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3055c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚ùÑÔ∏èüå°Ô∏èüî• Generating thermal variants...\\n\")\n",
    "\n",
    "# We'll fine-tune the trained model on thermally-augmented data\n",
    "thermal_models = {}\n",
    "\n",
    "for temp_variant in ['cold', 'normal', 'hot']:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training {temp_variant.upper()} variant\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Clone the trained model\n",
    "    model = TDNNGenerator(memory_depth=5, hidden1=32, hidden2=16, quantize=True).to(device)\n",
    "    model.load_state_dict(generator.state_dict())\n",
    "    \n",
    "    if temp_variant == 'normal':\n",
    "        # No additional training needed\n",
    "        thermal_models[temp_variant] = model\n",
    "        print(\"Using base trained model\")\n",
    "        continue\n",
    "    \n",
    "    # Apply thermal augmentation\n",
    "    y_train_thermal = add_thermal_drift(y_train, temp_variant)\n",
    "    \n",
    "    # Fine-tune for 10 epochs\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        total_loss = 0\n",
    "        for i in range(0, len(u_train) - BATCH_SIZE, BATCH_SIZE):\n",
    "            batch_u = u_train[i:i+BATCH_SIZE]\n",
    "            batch_y = y_train_thermal[i:i+BATCH_SIZE]\n",
    "            \n",
    "            dpd_out = model(batch_u)\n",
    "            fake_pa_out = pa_twin(dpd_out)\n",
    "            \n",
    "            spec_loss, evm, acpr = spectral_loss(fake_pa_out, batch_u)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            spec_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += spec_loss.item()\n",
    "        \n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            print(f\"  Epoch {epoch+1}/10: Loss={total_loss/(len(u_train)//BATCH_SIZE):.4f}\")\n",
    "    \n",
    "    thermal_models[temp_variant] = model\n",
    "    print(f\"‚úÖ {temp_variant.capitalize()} variant ready\")\n",
    "\n",
    "print(\"\\n‚úÖ All thermal variants generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68914a5f",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Export Weights to FPGA Format\n",
    "\n",
    "Export quantized weights as hex files for RTL synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dbe114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('rtl/weights', exist_ok=True)\n",
    "\n",
    "def export_weights_to_hex(model, variant_name):\n",
    "    \"\"\"Export model weights as Q1.15 hex files for FPGA\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"\\nüì§ Exporting {variant_name} weights...\")\n",
    "    \n",
    "    # Extract and quantize weights\n",
    "    fc1_w = model.fc1.weight.detach().cpu().numpy()\n",
    "    fc1_b = model.fc1.bias.detach().cpu().numpy()\n",
    "    fc2_w = model.fc2.weight.detach().cpu().numpy()\n",
    "    fc2_b = model.fc2.bias.detach().cpu().numpy()\n",
    "    fc3_w = model.fc3.weight.detach().cpu().numpy()\n",
    "    fc3_b = model.fc3.bias.detach().cpu().numpy()\n",
    "    \n",
    "    def to_q115(val):\n",
    "        \"\"\"Convert float to Q1.15 format (16-bit signed)\"\"\"\n",
    "        val_clipped = np.clip(val, -1.0, 1.0 - 1/32768)\n",
    "        q115 = np.int16(np.round(val_clipped * 32768))\n",
    "        return f\"{q115 & 0xFFFF:04x}\"\n",
    "    \n",
    "    # Write weights\n",
    "    weights = [\n",
    "        ('fc1_weights', fc1_w.flatten()),\n",
    "        ('fc1_bias', fc1_b.flatten()),\n",
    "        ('fc2_weights', fc2_w.flatten()),\n",
    "        ('fc2_bias', fc2_b.flatten()),\n",
    "        ('fc3_weights', fc3_w.flatten()),\n",
    "        ('fc3_bias', fc3_b.flatten())\n",
    "    ]\n",
    "    \n",
    "    for name, data in weights:\n",
    "        filename = f\"rtl/weights/{variant_name}_{name}.hex\"\n",
    "        with open(filename, 'w') as f:\n",
    "            for val in data:\n",
    "                f.write(to_q115(val) + '\\n')\n",
    "        print(f\"  ‚úì {filename} ({len(data)} values)\")\n",
    "    \n",
    "    print(f\"‚úÖ {variant_name} weights exported\")\n",
    "\n",
    "# Export all thermal variants\n",
    "for variant_name, model in thermal_models.items():\n",
    "    export_weights_to_hex(model, variant_name)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ ALL WEIGHTS EXPORTED TO rtl/weights/\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nFiles ready for FPGA synthesis:\")\n",
    "for variant in ['cold', 'normal', 'hot']:\n",
    "    print(f\"  {variant}_fc1_weights.hex, {variant}_fc1_bias.hex\")\n",
    "    print(f\"  {variant}_fc2_weights.hex, {variant}_fc2_bias.hex\")\n",
    "    print(f\"  {variant}_fc3_weights.hex, {variant}_fc3_bias.hex\")\n",
    "\n",
    "print(\"\\nüì¶ Ready to download and use in Vivado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd79c4fa",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Compare: No DPD vs Supervised vs GAN\n",
    "\n",
    "Quantitative comparison for the contest judges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0852d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä QUANTITATIVE COMPARISON FOR CONTEST JUDGES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "generator.eval()\n",
    "pa_twin.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # No DPD (baseline)\n",
    "    baseline_pa_out = pa_twin(u_val)\n",
    "    _, baseline_evm, baseline_acpr = spectral_loss(baseline_pa_out, u_val)\n",
    "    \n",
    "    # With DPD (our GAN-trained model)\n",
    "    dpd_out = generator(u_val)\n",
    "    dpd_pa_out = pa_twin(dpd_out)\n",
    "    _, dpd_evm, dpd_acpr = spectral_loss(dpd_pa_out, u_val)\n",
    "\n",
    "print(\"\\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\")\n",
    "print(\"‚îÇ Method              ‚îÇ ACPR (dBc)   ‚îÇ EVM (%)      ‚îÇ Improvement   ‚îÇ\")\n",
    "print(\"‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\")\n",
    "print(f\"‚îÇ No DPD (Baseline)   ‚îÇ {baseline_acpr.item():>11.1f}  ‚îÇ {baseline_evm.item():>11.2f}  ‚îÇ ---           ‚îÇ\")\n",
    "print(f\"‚îÇ GAN + Spectral Loss ‚îÇ {dpd_acpr.item():>11.1f}  ‚îÇ {dpd_evm.item():>11.2f}  ‚îÇ {baseline_acpr.item()-dpd_acpr.item():>+11.1f} dB ‚îÇ\")\n",
    "print(\"‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\")\n",
    "\n",
    "acpr_improvement = baseline_acpr.item() - dpd_acpr.item()\n",
    "evm_improvement = baseline_evm.item() - dpd_evm.item()\n",
    "\n",
    "print(f\"\\nüéØ Key Results:\")\n",
    "print(f\"   ACPR Improvement: {acpr_improvement:.1f} dB (lower spectral regrowth)\")\n",
    "print(f\"   EVM Improvement: {evm_improvement:.2f}% (better constellation quality)\")\n",
    "\n",
    "if acpr_improvement > 10 and evm_improvement > 3:\n",
    "    print(\"\\n‚úÖ EXCELLENT - Exceeds typical DPD performance!\")\n",
    "elif acpr_improvement > 5 and evm_improvement > 2:\n",
    "    print(\"\\n‚úÖ GOOD - Competitive DPD performance\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è May need more training or hyperparameter tuning\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac61fb5",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£3Ô∏è‚É£ Download Results for FPGA Validation\n",
    "\n",
    "Download the trained weights and use them in Vivado for RTL synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d6ef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create zip file of all weights\n",
    "import shutil\n",
    "\n",
    "print(\"üì¶ Creating weights package...\")\n",
    "shutil.make_archive('dpd_weights', 'zip', 'rtl/weights')\n",
    "print(\"‚úÖ Created dpd_weights.zip\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ TRAINING COMPLETE - READY FOR FPGA DEPLOYMENT\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nNext steps for RTL validation:\")\n",
    "print(\"1. Download dpd_weights.zip from Colab\")\n",
    "print(\"2. Extract to your project's rtl/weights/ directory\")\n",
    "print(\"3. Run: cd rtl && make vivado_pynq\")\n",
    "print(\"4. Load bitstream onto PYNQ-Z1 board\")\n",
    "print(\"5. Run demo: python demo/hdmi_demo.py\")\n",
    "print(\"\\nüìö See PROJECT_STATUS.md for detailed deployment guide\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Provide download link\n",
    "from google.colab import files\n",
    "print(\"\\n‚¨áÔ∏è Downloading weights package...\")\n",
    "files.download('dpd_weights.zip')\n",
    "print(\"‚úÖ Download started!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0c39fb",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bb8bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TDNNGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    Time-Delay Neural Network Generator for DPD.\n",
    "    Memory-aware architecture with envelope features.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=18, hidden_dims=[32, 16], output_dim=2, \n",
    "                 quantize=False, num_bits=16):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.quantize = quantize\n",
    "        self.num_bits = num_bits\n",
    "        \n",
    "        # Build layers\n",
    "        layers = []\n",
    "        in_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            layers.append(nn.LeakyReLU(0.25))  # 0.25 for easy shift in HW\n",
    "            in_dim = hidden_dim\n",
    "            \n",
    "        self.features = nn.Sequential(*layers)\n",
    "        self.output = nn.Linear(in_dim, output_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, a=0.25)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "                    \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, seq, input_dim] or [batch, input_dim]\n",
    "        if x.dim() == 3:\n",
    "            batch, seq, _ = x.shape\n",
    "            x = x.reshape(-1, self.input_dim)\n",
    "            reshape_back = True\n",
    "        else:\n",
    "            reshape_back = False\n",
    "            batch = x.shape[0]\n",
    "            seq = 1\n",
    "            \n",
    "        # Forward pass\n",
    "        h = self.features(x)\n",
    "        out = self.tanh(self.output(h))\n",
    "        \n",
    "        if reshape_back:\n",
    "            out = out.reshape(batch, seq, -1)\n",
    "            \n",
    "        return out\n",
    "\n",
    "# Count parameters\n",
    "model = TDNNGenerator(input_dim=18, hidden_dims=[32, 16], output_dim=2)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"TDNN Generator: {total_params:,} parameters\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name}: {list(param.shape)} = {param.numel()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb17aff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"CWGAN-GP Critic with spectral normalization.\"\"\"\n",
    "    def __init__(self, input_dim=4, hidden_dims=[64, 32, 16]):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        in_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.utils.spectral_norm(nn.Linear(in_dim, hidden_dim)))\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "            in_dim = hidden_dim\n",
    "            \n",
    "        self.features = nn.Sequential(*layers)\n",
    "        self.output = nn.utils.spectral_norm(nn.Linear(in_dim, 1))\n",
    "        \n",
    "    def forward(self, x, condition):\n",
    "        # Concatenate PA output and condition\n",
    "        combined = torch.cat([x, condition], dim=-1)\n",
    "        h = self.features(combined)\n",
    "        return self.output(h)\n",
    "\n",
    "disc = Discriminator()\n",
    "print(f\"Discriminator: {sum(p.numel() for p in disc.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce008ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PADigitalTwin(nn.Module):\n",
    "    \"\"\"\n",
    "    PA Digital Twin using Volterra model.\n",
    "    Models AM-AM, AM-PM, and memory effects.\n",
    "    \"\"\"\n",
    "    def __init__(self, memory_depth=5, nonlin_order=5):\n",
    "        super().__init__()\n",
    "        self.memory_depth = memory_depth\n",
    "        self.nonlin_order = nonlin_order\n",
    "        \n",
    "        # Volterra coefficients (learnable or fixed)\n",
    "        self.register_buffer('alpha1', torch.tensor(0.95))   # Linear gain\n",
    "        self.register_buffer('alpha3', torch.tensor(-0.12))  # 3rd order\n",
    "        self.register_buffer('alpha5', torch.tensor(0.03))   # 5th order\n",
    "        self.register_buffer('beta', torch.tensor(0.15))     # AM-PM\n",
    "        \n",
    "        # Memory coefficients\n",
    "        mem_coef = torch.tensor([1.0, 0.3, 0.1, 0.05, 0.02])\n",
    "        self.register_buffer('memory_coef', mem_coef / mem_coef.sum())\n",
    "        \n",
    "    def forward(self, x, temperature_state=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Complex input [batch, seq, 2] (I, Q)\n",
    "            temperature_state: 0=cold, 1=normal, 2=hot\n",
    "        \"\"\"\n",
    "        # Apply temperature drift\n",
    "        temp_scale = 1.0 + 0.05 * (temperature_state - 1)\n",
    "        \n",
    "        # Get I/Q components\n",
    "        x_i = x[..., 0]\n",
    "        x_q = x[..., 1]\n",
    "        \n",
    "        # Complex magnitude\n",
    "        mag_sq = x_i**2 + x_q**2\n",
    "        mag = torch.sqrt(mag_sq + 1e-8)\n",
    "        \n",
    "        # AM-AM: Polynomial compression\n",
    "        gain = self.alpha1 + self.alpha3 * mag_sq + self.alpha5 * mag_sq**2\n",
    "        gain = gain * temp_scale\n",
    "        \n",
    "        # AM-PM: Phase rotation\n",
    "        phase_shift = self.beta * mag_sq * temp_scale\n",
    "        cos_phi = torch.cos(phase_shift)\n",
    "        sin_phi = torch.sin(phase_shift)\n",
    "        \n",
    "        # Apply gain and phase\n",
    "        y_i = gain * (x_i * cos_phi - x_q * sin_phi)\n",
    "        y_q = gain * (x_i * sin_phi + x_q * cos_phi)\n",
    "        \n",
    "        # Memory effects (simplified FIR)\n",
    "        if x.dim() == 3 and x.shape[1] >= self.memory_depth:\n",
    "            y_i_mem = F.conv1d(\n",
    "                y_i.unsqueeze(1), \n",
    "                self.memory_coef.view(1, 1, -1),\n",
    "                padding=self.memory_depth // 2\n",
    "            ).squeeze(1)[..., :y_i.shape[-1]]\n",
    "            y_q_mem = F.conv1d(\n",
    "                y_q.unsqueeze(1),\n",
    "                self.memory_coef.view(1, 1, -1),\n",
    "                padding=self.memory_depth // 2\n",
    "            ).squeeze(1)[..., :y_q.shape[-1]]\n",
    "        else:\n",
    "            y_i_mem = y_i\n",
    "            y_q_mem = y_q\n",
    "            \n",
    "        return torch.stack([y_i_mem, y_q_mem], dim=-1)\n",
    "\n",
    "pa = PADigitalTwin()\n",
    "print(\"PA Digital Twin created (Volterra model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e070c1",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e6bd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_memory_features(x, memory_depth=5):\n",
    "    \"\"\"\n",
    "    Create input features for TDNN including memory taps and envelope.\n",
    "    \n",
    "    Input: x [batch, seq, 2] (I, Q)\n",
    "    Output: features [batch, seq, 18]\n",
    "        - Current I/Q: 2\n",
    "        - Delayed I/Q (3 taps): 6  \n",
    "        - Envelope features |x|^2, |x|^4 (5 taps): 10\n",
    "    \"\"\"\n",
    "    batch, seq, _ = x.shape\n",
    "    \n",
    "    # Pad for delays\n",
    "    x_padded = F.pad(x, (0, 0, memory_depth-1, 0), mode='replicate')\n",
    "    \n",
    "    features_list = []\n",
    "    \n",
    "    # Current I/Q\n",
    "    features_list.append(x)  # [batch, seq, 2]\n",
    "    \n",
    "    # Delayed I/Q (taps 1, 2, 3)\n",
    "    for d in range(1, 4):\n",
    "        delayed = x_padded[:, memory_depth-1-d:memory_depth-1-d+seq, :]\n",
    "        features_list.append(delayed)\n",
    "    \n",
    "    # Envelope features: |x[n-k]|^2 and |x[n-k]|^4 for k=0..4\n",
    "    for d in range(5):\n",
    "        if d == 0:\n",
    "            tap = x\n",
    "        else:\n",
    "            tap = x_padded[:, memory_depth-1-d:memory_depth-1-d+seq, :]\n",
    "        \n",
    "        mag_sq = tap[..., 0]**2 + tap[..., 1]**2  # |x|^2\n",
    "        mag_4 = mag_sq ** 2  # |x|^4\n",
    "        \n",
    "        features_list.append(mag_sq.unsqueeze(-1))\n",
    "        features_list.append(mag_4.unsqueeze(-1))\n",
    "    \n",
    "    # Concatenate all features\n",
    "    features = torch.cat(features_list, dim=-1)  # [batch, seq, 18]\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Test feature creation\n",
    "test_x = torch.randn(4, 64, 2)\n",
    "test_features = create_memory_features(test_x)\n",
    "print(f\"Input shape: {test_x.shape}\")\n",
    "print(f\"Features shape: {test_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eba15fd",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f21c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_evm(reference, signal):\n",
    "    \"\"\"Compute Error Vector Magnitude (lower is better).\"\"\"\n",
    "    error = signal - reference\n",
    "    error_power = (error ** 2).sum(dim=-1).mean()\n",
    "    ref_power = (reference ** 2).sum(dim=-1).mean()\n",
    "    evm = torch.sqrt(error_power / (ref_power + 1e-8))\n",
    "    return evm\n",
    "\n",
    "def compute_nmse(reference, signal):\n",
    "    \"\"\"Compute Normalized Mean Square Error.\"\"\"\n",
    "    error = signal - reference\n",
    "    nmse = (error ** 2).sum() / (reference ** 2).sum()\n",
    "    return nmse\n",
    "\n",
    "def compute_acpr(signal, main_bw_ratio=0.8, adj_bw_ratio=0.1):\n",
    "    \"\"\"Compute Adjacent Channel Power Ratio.\"\"\"\n",
    "    # Complex signal\n",
    "    sig_complex = signal[..., 0] + 1j * signal[..., 1]\n",
    "    \n",
    "    # FFT\n",
    "    spectrum = torch.fft.fft(sig_complex, dim=-1)\n",
    "    power = (spectrum.real**2 + spectrum.imag**2)\n",
    "    \n",
    "    n_fft = power.shape[-1]\n",
    "    \n",
    "    # Main channel (center)\n",
    "    main_start = int(n_fft * (0.5 - main_bw_ratio/2))\n",
    "    main_end = int(n_fft * (0.5 + main_bw_ratio/2))\n",
    "    main_power = power[..., main_start:main_end].sum()\n",
    "    \n",
    "    # Adjacent channels\n",
    "    adj_lower = power[..., :int(n_fft * adj_bw_ratio)].sum()\n",
    "    adj_upper = power[..., -int(n_fft * adj_bw_ratio):].sum()\n",
    "    adj_power = adj_lower + adj_upper\n",
    "    \n",
    "    acpr = 10 * torch.log10(adj_power / (main_power + 1e-8))\n",
    "    return acpr\n",
    "\n",
    "class SpectralLoss(nn.Module):\n",
    "    \"\"\"Combined spectral loss: EVM + ACPR + NMSE.\"\"\"\n",
    "    def __init__(self, evm_weight=1.0, acpr_weight=0.5, nmse_weight=1.0):\n",
    "        super().__init__()\n",
    "        self.evm_weight = evm_weight\n",
    "        self.acpr_weight = acpr_weight\n",
    "        self.nmse_weight = nmse_weight\n",
    "        \n",
    "    def forward(self, reference, signal):\n",
    "        evm = compute_evm(reference, signal)\n",
    "        nmse = compute_nmse(reference, signal)\n",
    "        acpr = compute_acpr(signal)\n",
    "        \n",
    "        # ACPR is in dB (negative), we want to minimize it (make more negative)\n",
    "        # So we add it (or use -acpr to maximize suppression)\n",
    "        loss = (self.evm_weight * evm + \n",
    "                self.nmse_weight * nmse - \n",
    "                self.acpr_weight * acpr / 50)  # Normalize ACPR contribution\n",
    "        \n",
    "        return loss, {'evm': evm.item(), 'nmse': nmse.item(), 'acpr': acpr.item()}\n",
    "\n",
    "spectral_loss = SpectralLoss()\n",
    "print(\"Spectral loss function created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd04885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(disc, real, fake, condition, device):\n",
    "    \"\"\"WGAN-GP gradient penalty.\"\"\"\n",
    "    alpha = torch.rand(real.size(0), 1, 1, device=device)\n",
    "    interpolates = alpha * real + (1 - alpha) * fake\n",
    "    interpolates.requires_grad_(True)\n",
    "    \n",
    "    d_interp = disc(interpolates, condition)\n",
    "    \n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interp,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=torch.ones_like(d_interp),\n",
    "        create_graph=True,\n",
    "        retain_graph=True\n",
    "    )[0]\n",
    "    \n",
    "    gradients = gradients.reshape(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    \n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6c53a7",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Generate Synthetic Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e125b393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ofdm_signal(batch_size, seq_len, num_subcarriers=64):\n",
    "    \"\"\"\n",
    "    Generate OFDM-like signal for training.\n",
    "    \"\"\"\n",
    "    # Random QAM symbols\n",
    "    qam_symbols = (torch.randint(0, 4, (batch_size, num_subcarriers)) * 2 - 3) + \\\n",
    "                  1j * (torch.randint(0, 4, (batch_size, num_subcarriers)) * 2 - 3)\n",
    "    qam_symbols = qam_symbols / torch.abs(qam_symbols).max()\n",
    "    \n",
    "    # IFFT to get time domain\n",
    "    time_signal = torch.fft.ifft(qam_symbols, n=seq_len, dim=-1)\n",
    "    \n",
    "    # Stack I/Q\n",
    "    signal = torch.stack([time_signal.real, time_signal.imag], dim=-1)\n",
    "    \n",
    "    # Normalize\n",
    "    signal = signal / (signal.abs().max() + 1e-8) * 0.8\n",
    "    \n",
    "    return signal.float()\n",
    "\n",
    "# Generate test batch\n",
    "test_signal = generate_ofdm_signal(8, 256)\n",
    "print(f\"Generated signal shape: {test_signal.shape}\")\n",
    "print(f\"Signal range: [{test_signal.min():.3f}, {test_signal.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee839bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize signal and PA distortion\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Time domain\n",
    "ax = axes[0, 0]\n",
    "ax.plot(test_signal[0, :100, 0].numpy(), label='I')\n",
    "ax.plot(test_signal[0, :100, 1].numpy(), label='Q')\n",
    "ax.set_title('Input Signal (Time Domain)')\n",
    "ax.legend()\n",
    "\n",
    "# Constellation\n",
    "ax = axes[0, 1]\n",
    "ax.scatter(test_signal[0, :, 0].numpy(), test_signal[0, :, 1].numpy(), \n",
    "           alpha=0.5, s=5)\n",
    "ax.set_title('Input Constellation')\n",
    "ax.set_xlabel('I')\n",
    "ax.set_ylabel('Q')\n",
    "ax.axis('equal')\n",
    "\n",
    "# PA output\n",
    "pa_output = pa(test_signal)\n",
    "ax = axes[1, 0]\n",
    "ax.plot(pa_output[0, :100, 0].detach().numpy(), label='I')\n",
    "ax.plot(pa_output[0, :100, 1].detach().numpy(), label='Q')\n",
    "ax.set_title('PA Output (Distorted)')\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[1, 1]\n",
    "ax.scatter(pa_output[0, :, 0].detach().numpy(), \n",
    "           pa_output[0, :, 1].detach().numpy(), \n",
    "           alpha=0.5, s=5)\n",
    "ax.set_title('PA Output Constellation (Distorted)')\n",
    "ax.set_xlabel('I')\n",
    "ax.set_ylabel('Q')\n",
    "ax.axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute distortion metrics\n",
    "evm = compute_evm(test_signal, pa_output)\n",
    "print(f\"\\nPA Distortion Metrics (without DPD):\")\n",
    "print(f\"  EVM: {evm.item()*100:.2f}%\")\n",
    "print(f\"  EVM (dB): {20*np.log10(evm.item()):.2f} dB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cc5eef",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2777d685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "SEQ_LEN = 256\n",
    "LR_G = 1e-4\n",
    "LR_D = 1e-4\n",
    "N_CRITIC = 5  # Train discriminator N times per generator update\n",
    "GP_WEIGHT = 10.0\n",
    "SPECTRAL_WEIGHT = 0.5\n",
    "\n",
    "# Initialize models\n",
    "generator = TDNNGenerator(input_dim=18, hidden_dims=[32, 16], output_dim=2).to(device)\n",
    "discriminator = Discriminator(input_dim=4, hidden_dims=[64, 32, 16]).to(device)\n",
    "pa_model = PADigitalTwin().to(device)\n",
    "\n",
    "# Optimizers\n",
    "opt_g = torch.optim.Adam(generator.parameters(), lr=LR_G, betas=(0.5, 0.9))\n",
    "opt_d = torch.optim.Adam(discriminator.parameters(), lr=LR_D, betas=(0.5, 0.9))\n",
    "\n",
    "# Loss\n",
    "spectral_loss_fn = SpectralLoss(evm_weight=1.0, acpr_weight=0.5, nmse_weight=1.0)\n",
    "\n",
    "print(\"Training setup complete!\")\n",
    "print(f\"  Generator params: {sum(p.numel() for p in generator.parameters()):,}\")\n",
    "print(f\"  Discriminator params: {sum(p.numel() for p in discriminator.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77449757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'd_loss': [], 'g_loss': [], 'evm': [], 'acpr': [], 'w_distance': []\n",
    "}\n",
    "\n",
    "# Training loop\n",
    "pbar = tqdm(range(NUM_EPOCHS), desc='Training')\n",
    "\n",
    "for epoch in pbar:\n",
    "    epoch_d_loss = 0\n",
    "    epoch_g_loss = 0\n",
    "    epoch_evm = 0\n",
    "    n_batches = 100  # Batches per epoch\n",
    "    \n",
    "    for batch_idx in range(n_batches):\n",
    "        # Generate training data\n",
    "        x_input = generate_ofdm_signal(BATCH_SIZE, SEQ_LEN).to(device)\n",
    "        \n",
    "        # Create memory features\n",
    "        features = create_memory_features(x_input)\n",
    "        \n",
    "        # ==================\n",
    "        # Train Discriminator\n",
    "        # ==================\n",
    "        for _ in range(N_CRITIC):\n",
    "            opt_d.zero_grad()\n",
    "            \n",
    "            # Generator output (predistorted signal)\n",
    "            with torch.no_grad():\n",
    "                dpd_output = generator(features)\n",
    "            \n",
    "            # PA output\n",
    "            pa_output = pa_model(dpd_output)\n",
    "            \n",
    "            # Real = ideal output (input signal)\n",
    "            # Fake = PA output after DPD\n",
    "            real_score = discriminator(x_input, x_input)\n",
    "            fake_score = discriminator(pa_output, x_input)\n",
    "            \n",
    "            # Wasserstein loss\n",
    "            d_loss = fake_score.mean() - real_score.mean()\n",
    "            \n",
    "            # Gradient penalty\n",
    "            gp = compute_gradient_penalty(discriminator, x_input, pa_output, x_input, device)\n",
    "            \n",
    "            # Total discriminator loss\n",
    "            d_total = d_loss + GP_WEIGHT * gp\n",
    "            d_total.backward()\n",
    "            opt_d.step()\n",
    "        \n",
    "        # ==================\n",
    "        # Train Generator\n",
    "        # ==================\n",
    "        opt_g.zero_grad()\n",
    "        \n",
    "        # Generate predistorted signal\n",
    "        dpd_output = generator(features)\n",
    "        \n",
    "        # PA output\n",
    "        pa_output = pa_model(dpd_output)\n",
    "        \n",
    "        # Adversarial loss (want discriminator to think PA output is real)\n",
    "        fake_score = discriminator(pa_output, x_input)\n",
    "        g_adv_loss = -fake_score.mean()\n",
    "        \n",
    "        # Spectral loss (EVM, ACPR)\n",
    "        g_spectral_loss, spectral_info = spectral_loss_fn(x_input, pa_output)\n",
    "        \n",
    "        # Total generator loss\n",
    "        g_total = g_adv_loss + SPECTRAL_WEIGHT * g_spectral_loss\n",
    "        g_total.backward()\n",
    "        opt_g.step()\n",
    "        \n",
    "        # Accumulate metrics\n",
    "        epoch_d_loss += d_loss.item()\n",
    "        epoch_g_loss += g_total.item()\n",
    "        epoch_evm += spectral_info['evm']\n",
    "    \n",
    "    # Average metrics\n",
    "    epoch_d_loss /= n_batches\n",
    "    epoch_g_loss /= n_batches\n",
    "    epoch_evm /= n_batches\n",
    "    \n",
    "    # Record history\n",
    "    history['d_loss'].append(epoch_d_loss)\n",
    "    history['g_loss'].append(epoch_g_loss)\n",
    "    history['evm'].append(epoch_evm)\n",
    "    history['w_distance'].append(-epoch_d_loss)\n",
    "    \n",
    "    # Update progress bar\n",
    "    pbar.set_postfix({\n",
    "        'D_loss': f'{epoch_d_loss:.4f}',\n",
    "        'G_loss': f'{epoch_g_loss:.4f}',\n",
    "        'EVM': f'{epoch_evm*100:.2f}%'\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5501fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "ax = axes[0, 0]\n",
    "ax.plot(history['d_loss'], label='Discriminator')\n",
    "ax.plot(history['g_loss'], label='Generator')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training Loss')\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[0, 1]\n",
    "ax.plot(history['w_distance'])\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Wasserstein Distance')\n",
    "ax.set_title('Wasserstein Distance')\n",
    "\n",
    "ax = axes[1, 0]\n",
    "ax.plot([e*100 for e in history['evm']])\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('EVM (%)')\n",
    "ax.set_title('Error Vector Magnitude')\n",
    "\n",
    "ax = axes[1, 1]\n",
    "ax.plot([20*np.log10(e) for e in history['evm']])\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('EVM (dB)')\n",
    "ax.set_title('EVM in dB (lower is better)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a134fb95",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b54296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "generator.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Generate test signal\n",
    "    test_input = generate_ofdm_signal(1, 1024).to(device)\n",
    "    test_features = create_memory_features(test_input)\n",
    "    \n",
    "    # Without DPD\n",
    "    pa_no_dpd = pa_model(test_input)\n",
    "    \n",
    "    # With DPD\n",
    "    dpd_output = generator(test_features)\n",
    "    pa_with_dpd = pa_model(dpd_output)\n",
    "\n",
    "# Compute metrics\n",
    "evm_no_dpd = compute_evm(test_input, pa_no_dpd)\n",
    "evm_with_dpd = compute_evm(test_input, pa_with_dpd)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"Final Evaluation Results\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nWithout DPD:\")\n",
    "print(f\"  EVM: {evm_no_dpd.item()*100:.2f}% ({20*np.log10(evm_no_dpd.item()):.2f} dB)\")\n",
    "print(f\"\\nWith DPD:\")\n",
    "print(f\"  EVM: {evm_with_dpd.item()*100:.2f}% ({20*np.log10(evm_with_dpd.item()):.2f} dB)\")\n",
    "print(f\"\\nImprovement: {(evm_no_dpd.item() - evm_with_dpd.item())*100:.2f}% absolute\")\n",
    "print(f\"             {20*np.log10(evm_no_dpd.item()/evm_with_dpd.item()):.2f} dB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeb780e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "test_input_np = test_input[0].cpu().numpy()\n",
    "pa_no_dpd_np = pa_no_dpd[0].cpu().numpy()\n",
    "pa_with_dpd_np = pa_with_dpd[0].cpu().numpy()\n",
    "\n",
    "# Constellation plots\n",
    "ax = axes[0, 0]\n",
    "ax.scatter(test_input_np[:, 0], test_input_np[:, 1], alpha=0.5, s=5)\n",
    "ax.set_title('Input Signal')\n",
    "ax.set_xlabel('I')\n",
    "ax.set_ylabel('Q')\n",
    "ax.axis('equal')\n",
    "\n",
    "ax = axes[0, 1]\n",
    "ax.scatter(pa_no_dpd_np[:, 0], pa_no_dpd_np[:, 1], alpha=0.5, s=5, c='red')\n",
    "ax.set_title(f'Without DPD (EVM={evm_no_dpd.item()*100:.1f}%)')\n",
    "ax.set_xlabel('I')\n",
    "ax.set_ylabel('Q')\n",
    "ax.axis('equal')\n",
    "\n",
    "ax = axes[0, 2]\n",
    "ax.scatter(pa_with_dpd_np[:, 0], pa_with_dpd_np[:, 1], alpha=0.5, s=5, c='green')\n",
    "ax.set_title(f'With DPD (EVM={evm_with_dpd.item()*100:.1f}%)')\n",
    "ax.set_xlabel('I')\n",
    "ax.set_ylabel('Q')\n",
    "ax.axis('equal')\n",
    "\n",
    "# Spectrum plots\n",
    "def plot_spectrum(ax, signal, title):\n",
    "    sig_complex = signal[:, 0] + 1j * signal[:, 1]\n",
    "    spectrum = np.fft.fftshift(np.fft.fft(sig_complex))\n",
    "    power_db = 20 * np.log10(np.abs(spectrum) + 1e-10)\n",
    "    freqs = np.fft.fftshift(np.fft.fftfreq(len(sig_complex)))\n",
    "    ax.plot(freqs, power_db)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Normalized Frequency')\n",
    "    ax.set_ylabel('Power (dB)')\n",
    "    ax.set_ylim(-60, 10)\n",
    "\n",
    "plot_spectrum(axes[1, 0], test_input_np, 'Input Spectrum')\n",
    "plot_spectrum(axes[1, 1], pa_no_dpd_np, 'Without DPD Spectrum')\n",
    "plot_spectrum(axes[1, 2], pa_with_dpd_np, 'With DPD Spectrum')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efeffdf",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Export Weights for FPGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b166247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_weights(weight, num_bits=16):\n",
    "    \"\"\"Quantize weights to fixed-point Q1.15 format.\"\"\"\n",
    "    scale = 2 ** (num_bits - 1) - 1\n",
    "    weight_clipped = torch.clamp(weight, -1.0, 1.0 - 1/scale)\n",
    "    weight_quantized = torch.round(weight_clipped * scale) / scale\n",
    "    return weight_quantized\n",
    "\n",
    "def export_weights_hex(model, filepath):\n",
    "    \"\"\"Export model weights to Verilog $readmemh format.\"\"\"\n",
    "    all_weights = []\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        w = param.detach().cpu()\n",
    "        w_quant = quantize_weights(w)\n",
    "        w_int = (w_quant * 32767).to(torch.int16)\n",
    "        all_weights.extend(w_int.flatten().tolist())\n",
    "        print(f\"  {name}: {list(w.shape)} = {w.numel()} params\")\n",
    "    \n",
    "    with open(filepath, 'w') as f:\n",
    "        f.write(f\"// TDNN Generator weights - {len(all_weights)} total\\n\")\n",
    "        f.write(f\"// Format: Q1.15 signed fixed-point\\n\\n\")\n",
    "        for w in all_weights:\n",
    "            if w < 0:\n",
    "                w = (1 << 16) + w\n",
    "            f.write(f\"{w:04X}\\n\")\n",
    "    \n",
    "    print(f\"\\nExported {len(all_weights)} weights to {filepath}\")\n",
    "\n",
    "# Export\n",
    "print(\"Exporting weights...\")\n",
    "export_weights_hex(generator, 'weights_trained.hex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b882575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save checkpoint for later use\n",
    "checkpoint = {\n",
    "    'epoch': NUM_EPOCHS,\n",
    "    'generator_state_dict': generator.state_dict(),\n",
    "    'discriminator_state_dict': discriminator.state_dict(),\n",
    "    'optimizer_g_state_dict': opt_g.state_dict(),\n",
    "    'optimizer_d_state_dict': opt_d.state_dict(),\n",
    "    'history': history,\n",
    "    'config': {\n",
    "        'input_dim': 18,\n",
    "        'hidden_dims': [32, 16],\n",
    "        'output_dim': 2,\n",
    "    }\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, 'dpd_trained.pt')\n",
    "print(\"Checkpoint saved to dpd_trained.pt\")\n",
    "\n",
    "# Download files (for Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('weights_trained.hex')\n",
    "    files.download('dpd_trained.pt')\n",
    "    print(\"Files downloaded!\")\n",
    "except:\n",
    "    print(\"Not running on Colab - files saved locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f412c3f5",
   "metadata": {},
   "source": [
    "## üéØ Summary\n",
    "\n",
    "### Trained Model\n",
    "- **Architecture**: TDNN 18‚Üí32‚Üí16‚Üí2\n",
    "- **Parameters**: 1,170 (fits in ~2.4KB)\n",
    "- **Quantization**: Q1.15 (16-bit fixed-point)\n",
    "\n",
    "### Next Steps\n",
    "1. Download `weights_trained.hex`\n",
    "2. Copy to `rtl/weights/` folder\n",
    "3. Run RTL simulation: `cd rtl && make sim_all`\n",
    "4. Build FPGA bitstream in Vivado\n",
    "5. Run HDMI demo on PYNQ-Z1/ZCU104\n",
    "\n",
    "### Files Generated\n",
    "- `weights_trained.hex` - Verilog weight file\n",
    "- `dpd_trained.pt` - PyTorch checkpoint"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
