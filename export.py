#!/usr/bin/env python3
"""
Weight Export Script for 6G PA DPD System

Exports trained PyTorch TDNN weights to:
1. Verilog $readmemh format for RTL simulation
2. Binary format for FPGA BRAM initialization
3. C header files for embedded software
"""

import argparse
import numpy as np
import torch
import yaml
from pathlib import Path
import struct

from models import TDNNGenerator
from utils.quantization import quantize_weights_fixed_point


def load_checkpoint(checkpoint_path: str, config: dict) -> TDNNGenerator:
    """Load trained model from checkpoint."""
    model = TDNNGenerator(
        memory_depth=config['model']['generator'].get('memory_depth', 5),
        hidden_dims=config['model']['generator'].get('hidden_dims', [32, 16]),
        leaky_slope=config['model']['generator'].get('leaky_slope', 0.2)
    )
    
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    model.load_state_dict(checkpoint['generator_state_dict'])
    model.eval()
    
    return model


def export_verilog_hex(weights: np.ndarray, filepath: Path, 
                       bits: int = 16, values_per_line: int = 1):
    """Export weights to Verilog $readmemh format."""
    filepath.parent.mkdir(parents=True, exist_ok=True)
    
    with open(filepath, 'w') as f:
        f.write(f"// Weight file: {filepath.name}\n")
        f.write(f"// Total weights: {len(weights)}\n")
        f.write(f"// Format: Q1.{bits-1} signed fixed-point\n")
        f.write(f"// Generated by export.py\n\n")
        
        for i, w in enumerate(weights.flatten()):
            # Convert to signed fixed-point integer
            w_int = int(w)
            
            # Handle negative numbers (2's complement)
            if w_int < 0:
                w_int = (1 << bits) + w_int
            
            # Write hex value
            hex_str = format(w_int & ((1 << bits) - 1), f'0{bits//4}X')
            f.write(f"{hex_str}\n")
    
    print(f"Exported {len(weights.flatten())} weights to {filepath}")


def export_binary(weights: np.ndarray, filepath: Path, bits: int = 16):
    """Export weights to binary format for BRAM initialization."""
    filepath.parent.mkdir(parents=True, exist_ok=True)
    
    with open(filepath, 'wb') as f:
        for w in weights.flatten():
            w_int = int(w)
            if bits == 16:
                # Pack as signed 16-bit
                f.write(struct.pack('<h', w_int))
            elif bits == 8:
                f.write(struct.pack('<b', w_int))
            elif bits == 32:
                f.write(struct.pack('<i', w_int))
    
    print(f"Exported binary weights to {filepath}")


def export_c_header(weights: dict, filepath: Path, bits: int = 16):
    """Export weights as C header file for embedded software."""
    filepath.parent.mkdir(parents=True, exist_ok=True)
    
    with open(filepath, 'w') as f:
        f.write("// Auto-generated DPD weight header\n")
        f.write("// Do not edit manually\n\n")
        f.write("#ifndef DPD_WEIGHTS_H\n")
        f.write("#define DPD_WEIGHTS_H\n\n")
        f.write("#include <stdint.h>\n\n")
        
        type_str = f"int{bits}_t"
        
        for name, w in weights.items():
            w_flat = w.flatten()
            safe_name = name.replace('.', '_')
            
            f.write(f"// {name}: shape {w.shape}\n")
            f.write(f"static const {type_str} {safe_name}[{len(w_flat)}] = {{\n")
            
            # Write values
            for i, val in enumerate(w_flat):
                val_int = int(val)
                if i % 8 == 0:
                    f.write("    ")
                f.write(f"{val_int:6d}")
                if i < len(w_flat) - 1:
                    f.write(", ")
                if (i + 1) % 8 == 0:
                    f.write("\n")
            
            f.write("\n};\n\n")
        
        f.write("#endif // DPD_WEIGHTS_H\n")
    
    print(f"Exported C header to {filepath}")


def export_temperature_banks(model: TDNNGenerator, base_path: Path, 
                             config: dict) -> dict:
    """
    Export weight banks for different temperature states.
    
    For cold/hot, applies scaling factors from config.
    """
    bits = config['quantization']['weight_bits']
    temp_config = config['pa']
    
    banks = {
        'normal': {},
        'cold': {},
        'hot': {}
    }
    
    # Extract base weights
    for name, param in model.named_parameters():
        if 'weight' in name or 'bias' in name:
            w_float = param.detach().cpu().numpy()
            w_fixed = quantize_weights_fixed_point(
                torch.tensor(w_float), 
                num_bits=bits
            ).numpy()
            banks['normal'][name] = w_fixed
    
    # Apply temperature drift coefficients
    cold_scale = 1.0 + temp_config['cold_drift_factor']
    hot_scale = 1.0 + temp_config['hot_drift_factor']
    
    for name, w in banks['normal'].items():
        # Scale weights for temperature compensation
        banks['cold'][name] = (w * cold_scale).astype(np.int16)
        banks['hot'][name] = (w * hot_scale).astype(np.int16)
    
    # Export each bank
    for temp, weights in banks.items():
        bank_path = base_path / temp
        
        # Combined weight file for RTL
        all_weights = []
        for name in sorted(weights.keys()):
            all_weights.extend(weights[name].flatten())
        
        export_verilog_hex(
            np.array(all_weights), 
            bank_path / 'weights.hex',
            bits=bits
        )
        
        export_binary(
            np.array(all_weights),
            bank_path / 'weights.bin',
            bits=bits
        )
    
    return banks


def main():
    parser = argparse.ArgumentParser(
        description='Export trained DPD weights for FPGA deployment'
    )
    parser.add_argument(
        '--checkpoint', '-c',
        type=str,
        help='Path to trained model checkpoint (for single network or normal temp)'
    )
    parser.add_argument(
        '--checkpoint-cold',
        type=str,
        help='Path to cold temperature trained checkpoint'
    )
    parser.add_argument(
        '--checkpoint-normal',
        type=str,
        help='Path to normal temperature trained checkpoint'
    )
    parser.add_argument(
        '--checkpoint-hot',
        type=str,
        help='Path to hot temperature trained checkpoint'
    )
    parser.add_argument(
        '--config', '-f',
        type=str,
        default='config/config.yaml',
        help='Path to config file'
    )
    parser.add_argument(
        '--output', '-o',
        type=str,
        default='rtl/weights',
        help='Output directory for weight files'
    )
    parser.add_argument(
        '--format',
        type=str,
        nargs='+',
        default=['hex', 'bin', 'header'],
        choices=['hex', 'bin', 'header', 'all'],
        help='Export formats'
    )
    parser.add_argument(
        '--temperature-banks',
        action='store_true',
        help='Export separate weight banks for cold/normal/hot (applies thermal scaling)'
    )
    parser.add_argument(
        '--triple-trained',
        action='store_true',
        help='Use 3 separately trained checkpoints (requires --checkpoint-cold/normal/hot)'
    )
    args = parser.parse_args()
    
    # Validate arguments
    if args.triple_trained:
        if not all([args.checkpoint_cold, args.checkpoint_normal, args.checkpoint_hot]):
            parser.error("--triple-trained requires --checkpoint-cold, --checkpoint-normal, and --checkpoint-hot")
    else:
        if not args.checkpoint:
            parser.error("Either provide --checkpoint or use --triple-trained with 3 checkpoints")
    
    # Load config
    with open(args.config, 'r') as f:
        config = yaml.safe_load(f)
    
    output_path = Path(args.output)
    bits = config['quantization']['weight_bits']
    
    if 'all' in args.format:
        args.format = ['hex', 'bin', 'header']
    
    if args.triple_trained:
        # Load 3 separately trained models
        print("\n" + "="*60)
        print("TRIPLE TRAINING MODE: Loading 3 separately trained networks")
        print("="*60)
        
        print(f"\nLoading cold checkpoint: {args.checkpoint_cold}")
        model_cold = load_checkpoint(args.checkpoint_cold, config)
        
        print(f"Loading normal checkpoint: {args.checkpoint_normal}")
        model_normal = load_checkpoint(args.checkpoint_normal, config)
        
        print(f"Loading hot checkpoint: {args.checkpoint_hot}")
        model_hot = load_checkpoint(args.checkpoint_hot, config)
        
        models = {
            'cold': model_cold,
            'normal': model_normal,
            'hot': model_hot
        }
        
        # Export each bank separately
        for temp_name, model in models.items():
            bank_id = {'cold': 0, 'normal': 1, 'hot': 2}[temp_name]
            print(f"\n--- Exporting Bank {bank_id} ({temp_name.upper()}) ---")
            
            # Extract and quantize weights
            all_weights = []
            weights = {}
            
            for name, param in model.named_parameters():
                if 'weight' in name or 'bias' in name:
                    w_float = param.detach().cpu().numpy()
                    w_fixed = quantize_weights_fixed_point(
                        torch.tensor(w_float),
                        num_bits=bits
                    ).numpy()
                    weights[name] = w_fixed
                    all_weights.extend(w_fixed.flatten())
            
            all_weights = np.array(all_weights)
            
            # Export to files with bank suffix
            if 'hex' in args.format:
                export_verilog_hex(all_weights, output_path / f'weights_bank{bank_id}_{temp_name}.hex', bits)
            
            if 'bin' in args.format:
                export_binary(all_weights, output_path / f'weights_bank{bank_id}_{temp_name}.bin', bits)
            
            if 'header' in args.format:
                export_c_header(weights, output_path / f'dpd_weights_bank{bank_id}_{temp_name}.h', bits)
        
        print("\n" + "="*60)
        print("TRIPLE TRAINING EXPORT COMPLETE")
        print("="*60)
        print(f"✅ Exported 3 independently trained weight banks")
        print(f"   Bank 0 (Cold):   weights_bank0_cold.hex")
        print(f"   Bank 1 (Normal): weights_bank1_normal.hex")
        print(f"   Bank 2 (Hot):    weights_bank2_hot.hex")
        print(f"✅ Total BRAM: {3 * len(all_weights) * bits // 8} bytes (9.3 KB)")
        
    elif args.temperature_banks:
        # Single model with thermal scaling (legacy approach)
        print("\n" + "="*60)
        print("THERMAL SCALING MODE: Applying drift to single network")
        print("="*60)
        
        print(f"\nLoading checkpoint: {args.checkpoint}")
        model = load_checkpoint(args.checkpoint, config)
        
        # Export temperature-specific weight banks (apply thermal drift scaling)
        banks = export_temperature_banks(model, output_path, config)
        
        for temp_name in ['cold', 'normal', 'hot']:
            bank_id = {'cold': 0, 'normal': 1, 'hot': 2}[temp_name]
            all_weights = []
            
            for name in sorted(banks[temp_name].keys()):
                all_weights.extend(banks[temp_name][name].flatten())
            
            all_weights = np.array(all_weights)
            
            if 'hex' in args.format:
                export_verilog_hex(
                    all_weights, 
                    output_path / f'weights_bank{bank_id}_{temp_name}_scaled.hex', 
                    bits
                )
        
        print(f"\n⚠️  Using thermal scaling (linear drift approximation)")
        print(f"   Consider --triple-trained for better accuracy")
        
    else:
        # Single weight file export (no temperature banks)
        print(f"\nLoading checkpoint: {args.checkpoint}")
        model = load_checkpoint(args.checkpoint, config)
        
        # Export single weight set
        weights = {}
        all_weights = []
        
        for name, param in model.named_parameters():
            if 'weight' in name or 'bias' in name:
                w_float = param.detach().cpu().numpy()
                w_fixed = quantize_weights_fixed_point(
                    torch.tensor(w_float),
                    num_bits=bits
                ).numpy()
                weights[name] = w_fixed
                all_weights.extend(w_fixed.flatten())
        
        all_weights = np.array(all_weights)
        
        if 'hex' in args.format:
            export_verilog_hex(all_weights, output_path / 'weights.hex', bits)
        
        if 'bin' in args.format:
            export_binary(all_weights, output_path / 'weights.bin', bits)
        
        if 'header' in args.format:
            export_c_header(weights, output_path / 'dpd_weights.h', bits)
    
    # Print summary
    print("\n" + "=" * 60)
    print("Export Summary")
    print("=" * 60)
    
    # Use last loaded model for parameter count
    if args.triple_trained:
        model = models['normal']  # Use normal temp model for summary
    
    total_params = sum(p.numel() for p in model.parameters())
    print(f"Total parameters: {total_params}")
    print(f"Weight bits: {bits}")
    print(f"Memory size per bank: {total_params * bits // 8} bytes")
    print(f"Output directory: {output_path}")
    
    # Layer breakdown
    print("\nLayer breakdown:")
    for name, param in model.named_parameters():
        print(f"  {name}: {list(param.shape)} = {param.numel()}")


if __name__ == '__main__':
    main()
